{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from gensim.corpora import WikiCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读进所有文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/Jesica/Documents/igolden/word2vec/word2vec-tutorial-master/news'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e636cf7ad479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Select only files with the ext extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtxt_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Iterate over your txt files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/Jesica/Documents/igolden/word2vec/word2vec-tutorial-master/news'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "direc = '/Users/Jesica/Documents/igolden/word2vec/word2vec-tutorial-master/news' # Get current working directory\n",
    "ext = '.txt' # Select your file delimiter\n",
    "\n",
    "file_dict = {} # Create an empty dict\n",
    "txt = ''\n",
    "# Select only files with the ext extension\n",
    "txt_files = [i for i in os.listdir(direc) if os.path.splitext(i)[1] == ext]\n",
    "\n",
    "# Iterate over your txt files\n",
    "for f in txt_files:\n",
    "    # Open them and assign them to file_dict\n",
    "    with open(os.path.join(direc,f)) as file_object:\n",
    "        text = file_object.read()\n",
    "        file_dict[f] = text\n",
    "        txt = txt + '\\n'+ text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "句子分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_split(str_centence):\n",
    "    list_ret = list()\n",
    "    for s_str in str_centence.split('。'):\n",
    "        if '?' in s_str:\n",
    "            list_ret.extend(s_str.split('？'))\n",
    "        elif '!' in s_str:\n",
    "            list_ret.extend(s_str.split('！'))\n",
    "        else:\n",
    "            list_ret.append(s_str.strip())\n",
    "    return list_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = sentence_split(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "句子分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordset = set()\n",
    "with open('jieba_dict/stopwords.txt','r',encoding = 'utf-8') as sw:\n",
    "    for line in sw:\n",
    "        stopwordset.add(line.strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('word_seg.txt','w')\n",
    "texts_num = 0\n",
    "for line in l:\n",
    "    words = jieba.cut(line, cut_all = False)\n",
    "    for word in words:\n",
    "        if word not in stopwordset:\n",
    "            output.write(word + ' ')\n",
    "    texts_num += 1\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math  \n",
    "from string import punctuation  \n",
    "from heapq import nlargest  \n",
    "from itertools import product, count  \n",
    "from gensim.models import word2vec  \n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = word2vec.Text8Corpus('word_seg.txt')\n",
    "model = word2vec.Word2Vec(sentences, size =20)\n",
    "model.save('w2v100.model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2 = word2vec.Word2Vec(sentences, size=8, sg=1, hs=1, iter=10)\n",
    "#model2.save('w2v_sg_5.model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.most_similar('保险',topn = 20)\n",
    "for item in res:\n",
    "    print(item[0]+\",\"+str(item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sentences(sentence):  \n",
    "    puns = frozenset(u'。！？')  \n",
    "    tmp = []  \n",
    "    for ch in sentence:  \n",
    "        tmp.append(ch)  \n",
    "        if puns.__contains__(ch):  \n",
    "            yield ''.join(tmp)  \n",
    "            tmp = []  \n",
    "    yield ''.join(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_sentences_similarity(sents_1, sents_2):  \n",
    "    ''''' \n",
    "    计算两个句子的相似性 \n",
    "    相同词语的百分比\n",
    "    :param sents_1: \n",
    "    :param sents_2: \n",
    "    :return: \n",
    "    '''  \n",
    "    counter = 0  \n",
    "    for sent in sents_1:  \n",
    "        if sent in sents_2:  \n",
    "            counter += 1  \n",
    "    return counter / (math.log(len(sents_1) + len(sents_2)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(word_sent):  \n",
    "    \"\"\" \n",
    "    传入句子链表  返回句子之间相似度的图 \n",
    "    :param word_sent: \n",
    "    :return: \n",
    "    \"\"\"  \n",
    "    num = len(word_sent)  \n",
    "    board = [[0.0 for _ in range(num)] for _ in range(num)]  \n",
    "  \n",
    "    for i, j in product(range(num), repeat=2):  \n",
    "        if i != j:  \n",
    "            \n",
    "            board[i][j] = compute_similarity_by_avg(word_sent[i], word_sent[j])  \n",
    "    return board  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):  \n",
    "    ''''' \n",
    "    计算两个向量之间的余弦相似度 \n",
    "    :param vec1: \n",
    "    :param vec2: \n",
    "    :return: \n",
    "    '''  \n",
    "    tx = np.array(vec1)  \n",
    "    ty = np.array(vec2)  \n",
    "    cos1 = np.sum(tx * ty)  \n",
    "    cos21 = np.sqrt(sum(tx ** 2))  \n",
    "    cos22 = np.sqrt(sum(ty ** 2))  \n",
    "    cosine_value = cos1 / float(cos21 * cos22)  \n",
    "    return cosine_value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_by_avg(sents_1, sents_2):  \n",
    "    ''''' \n",
    "    对两个句子求平均词向量 \n",
    "    :param sents_1: \n",
    "    :param sents_2: \n",
    "    :return: \n",
    "    '''  \n",
    "    if len(sents_1) == 0 or len(sents_2) == 0:  \n",
    "        return 0.0  \n",
    "    vec1 = model[sents_1[0]]  \n",
    "    for word1 in sents_1[1:]:  \n",
    "        vec1 = vec1 + model[word1]  \n",
    "  \n",
    "    vec2 = model[sents_2[0]]  \n",
    "    for word2 in sents_2[1:]:  \n",
    "        print\n",
    "        vec2 = vec2 + model[word2]  \n",
    "  \n",
    "    similarity = cosine_similarity(vec1 / len(sents_1), vec2 / len(sents_2))  \n",
    "    return similarity  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(weight_graph, scores, i):  \n",
    "    \"\"\" \n",
    "    计算句子在图中的分数 \n",
    "    :param weight_graph: \n",
    "    :param scores: \n",
    "    :param i: \n",
    "    :return: \n",
    "    \"\"\"  \n",
    "    length = len(weight_graph)  \n",
    "    d = 0.85  \n",
    "    added_score = 0.0  \n",
    "  \n",
    "    for j in range(length):  \n",
    "        fraction = 0.0  \n",
    "        denominator = 0.0  \n",
    "        # 计算分子  \n",
    "        fraction = weight_graph[j][i] * scores[j]  \n",
    "        # 计算分母  \n",
    "        for k in range(length):  \n",
    "            denominator += weight_graph[j][k]  \n",
    "            if denominator == 0:  \n",
    "                denominator = 1  \n",
    "        added_score += fraction / denominator  \n",
    "    # 算出最终的分数  \n",
    "    weighted_score = (1 - d) + d * added_score  \n",
    "    return weighted_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_sentences_rank(weight_graph):  \n",
    "    ''''' \n",
    "    输入相似度的图（矩阵) \n",
    "    返回各个句子的分数 \n",
    "    :param weight_graph: \n",
    "    :return: \n",
    "    '''  \n",
    "    # 初始分数设置为0.5  \n",
    "    scores = [0.5 for _ in range(len(weight_graph))]  \n",
    "    old_scores = [0.0 for _ in range(len(weight_graph))]  \n",
    "  \n",
    "    # 开始迭代  \n",
    "    while different(scores, old_scores):  \n",
    "        for i in range(len(weight_graph)):  \n",
    "            old_scores[i] = scores[i]  \n",
    "        for i in range(len(weight_graph)):  \n",
    "            scores[i] = calculate_score(weight_graph, scores, i)  \n",
    "    return scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def different(scores, old_scores):  \n",
    "    ''''' \n",
    "    判断前后分数有无变化 \n",
    "    :param scores: \n",
    "    :param old_scores: \n",
    "    :return: \n",
    "    '''  \n",
    "    flag = False  \n",
    "    for i in range(len(scores)):  \n",
    "        if math.fabs(scores[i] - old_scores[i]) >= 0.0001:  \n",
    "            flag = True  \n",
    "            break  \n",
    "    return flag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_symbols(sents):  \n",
    "    stopwords = list(stopwordset)+ ['。', ' ', '.',', ','印']  \n",
    "    _sents = []  \n",
    "    for sentence in sents:  \n",
    "        _sent = []\n",
    "        for word in sentence:  \n",
    "            if word not in stopwordset:  \n",
    "                _sent.append(word)\n",
    "        if _sent:  \n",
    "            _sents.append(_sent)  \n",
    "    return _sents  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_model(sents):  \n",
    "    _sents = []  \n",
    "    for sentence in sents:  \n",
    "        _sent = []\n",
    "        for word in sentence:  \n",
    "            if word in model: \n",
    "                _sent.append(word) \n",
    "\n",
    "        if _sent:  \n",
    "            _sents.append(_sent)  \n",
    "    return _sents  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, n=0):  \n",
    "    text = text.replace('\\n','')\n",
    "    tokens = cut_sentences(text) \n",
    "    s_count = 0\n",
    "    sentences = []  \n",
    "    sents = []  \n",
    "    ratio = 0.1 #取文本10%\n",
    "    for sent in tokens:  \n",
    "        s_count = s_count + 1\n",
    "        sentences.append(sent)  \n",
    "        sents.append([word for word in jieba.cut(sent) if word ])  \n",
    "#    print(sents)\n",
    "    sents_fs = filter_symbols(sents)  \n",
    "\n",
    "    sents_fm = filter_model(sents_fs)  \n",
    "    graph = create_graph(sents_fm)  \n",
    "    if n==0:\n",
    "        n = math.floor(s_count*ratio)\n",
    "\n",
    "    scores = weight_sentences_rank(graph)  \n",
    "    sent_selected = nlargest(n, zip(scores, count()))  \n",
    "    sent_index = []  \n",
    "\n",
    "    for i in range(n):  \n",
    "        sent_index.append(sent_selected[i][1])  \n",
    "    #return [sentences[i] for i in sent_index]  \n",
    "    return [(sentences[i]) for i in sent_index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':  \n",
    "    #from IPython.core.debugger import Tracer; Tracer()()\n",
    "    with open(\"test/令人拍手叫好的年度幽默硬科幻巨作《火星救援》当中，主人公马特达蒙饰演的角色在火星上非常努力，种土豆养\"\n",
    "              , \"r\", encoding='utf-8') as myfile:  \n",
    "        text = myfile.read().replace('\\n', '')  \n",
    "        \n",
    "        print(summarize(text)) \n",
    "        print(summarize(text,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
